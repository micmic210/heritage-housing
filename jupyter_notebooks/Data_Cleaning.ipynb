{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "- Assess and handle missing values\n",
        "- Clean data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- outputs/datasets/collection/HousePrices.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "- Cleaned full dataset: outputs/datasets/cleaned/HousePricesCleaned.csv\n",
        "- Cleaned train/test splits: outputs/datasets/cleaned/TrainSetCleaned.csv, outputs/datasets/cleaned/TestSetCleaned.csv\n",
        "- Data cleaning pipeline: outputs/ml_pipeline/data_cleaning/dataCleaning_pipeline.pkl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "os.chdir(os.path.dirname(current_dir))  # set project root\n",
        "print(\"Current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load Collected Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HousePrices.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identify columns with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing = df.columns[df.isna().sum() > 0].tolist()\n",
        "print(\"Columns with missing:\", vars_with_missing)\n",
        "print(df[vars_with_missing].info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profile Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"There are no variables with missing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    \"\"\"\n",
        "    Function to evaluate data with missing values\n",
        "    \"\"\"\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute / len(df) * 100, 2)\n",
        "    df_missing_data = (\n",
        "        pd.DataFrame(\n",
        "            data={\n",
        "                \"RowsWithMissingData\": missing_data_absolute,\n",
        "                \"PercentageOfDataset\": missing_data_percentage,\n",
        "                \"DataType\": df.dtypes,\n",
        "            }\n",
        "        )\n",
        "        .sort_values(by=[\"PercentageOfDataset\"], ascending=False)\n",
        "        .query(\"PercentageOfDataset > 0\")\n",
        "    )\n",
        "\n",
        "    return df_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def DataCleaningEffect(df_original, df_cleaned, variables_applied_with_method):\n",
        "    \"\"\"\n",
        "    Function to visualize data cleaning effect\n",
        "    \"\"\"\n",
        "    flag_count = 1  # Indicate plot number\n",
        "\n",
        "    # distinguish between numerical and categorical variables\n",
        "    categorical_variables = df_original.select_dtypes(exclude=[\"number\"]).columns\n",
        "\n",
        "    # scan over variables,\n",
        "    # first on variables that you applied the method\n",
        "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
        "    for set_of_variables in [variables_applied_with_method]:\n",
        "        print(\n",
        "            \"\\n=====================================================================================\"\n",
        "        )\n",
        "        print(\n",
        "            f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\"\n",
        "        )\n",
        "        print(f\"{set_of_variables} \\n\\n\")\n",
        "\n",
        "        for var in set_of_variables:\n",
        "            if var in categorical_variables:  # it is categorical variable: barplot\n",
        "\n",
        "                df1 = pd.DataFrame({\"Type\": \"Original\", \"Value\": df_original[var]})\n",
        "                df2 = pd.DataFrame({\"Type\": \"Cleaned\", \"Value\": df_cleaned[var]})\n",
        "                dfAux = pd.concat([df1, df2], axis=0)\n",
        "                fig, axes = plt.subplots(figsize=(15, 5))\n",
        "                sns.countplot(\n",
        "                    hue=\"Type\", data=dfAux, x=\"Value\", palette=[\"#432371\", \"#FAAE7B\"]\n",
        "                )\n",
        "                axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "                plt.xticks(rotation=90)\n",
        "                plt.legend()\n",
        "\n",
        "            else:  # it is numerical variable: histogram\n",
        "\n",
        "                fig, axes = plt.subplots(figsize=(10, 5))\n",
        "                sns.histplot(\n",
        "                    data=df_original,\n",
        "                    x=var,\n",
        "                    color=\"#432371\",\n",
        "                    label=\"Original\",\n",
        "                    kde=True,\n",
        "                    element=\"step\",\n",
        "                    ax=axes,\n",
        "                )\n",
        "                sns.histplot(\n",
        "                    data=df_cleaned,\n",
        "                    x=var,\n",
        "                    color=\"#FAAE7B\",\n",
        "                    label=\"Cleaned\",\n",
        "                    kde=True,\n",
        "                    element=\"step\",\n",
        "                    ax=axes,\n",
        "                )\n",
        "                axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "                plt.legend()\n",
        "\n",
        "            plt.show()\n",
        "            flag_count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "    df, df[\"SalePrice\"], test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "variables_to_drop = [\"EnclosedPorch\", \"WoodDeckSF\"]\n",
        "imputer = DropFeatures(features_to_drop=variables_to_drop)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "\n",
        "for i in variables_to_drop:\n",
        "    print(i in df_method.columns.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "variables_mean = [\"LotFrontage\", \"BedroomAbvGr\"]\n",
        "imputer = MeanMedianImputer(imputation_method=\"mean\", variables=variables_mean)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(\n",
        "    df_original=TrainSet,\n",
        "    df_cleaned=df_method,\n",
        "    variables_applied_with_method=variables_mean,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Median Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_median = [\"2ndFlrSF\", \"GarageYrBlt\", \"MasVnrArea\"]\n",
        "imputer = MeanMedianImputer(imputation_method=\"median\", variables=variables_median)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(\n",
        "    df_original=TrainSet,\n",
        "    df_cleaned=df_method,\n",
        "    variables_applied_with_method=variables_median,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet[\"GarageArea\"] == 0)][[\"GarageYrBlt\", \"GarageArea\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "variables_categorical = [\"GarageFinish\", \"BsmtFinType1\"]\n",
        "imputer = CategoricalImputer(\n",
        "    imputation_method=\"missing\", fill_value=\"None\", variables=variables_categorical\n",
        ")\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(\n",
        "    df_original=TrainSet,\n",
        "    df_cleaned=df_method,\n",
        "    variables_applied_with_method=variables_categorical,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet[\"GarageArea\"] == 0)][[\"GarageFinish\", \"GarageArea\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet[\"TotalBsmtSF\"] == 0)][[\"BsmtFinType1\", \"TotalBsmtSF\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pipeline has the following steps:\n",
        "\n",
        "- Mean imputation: variables=['LotFrontage' , 'BedroomAbvGr']\n",
        "- Median imputation: variables=['2ndFlrSF', 'MasVnrArea']\n",
        "- Categorical imputation: variables=['GarageFinish' , 'BsmtFinType1']\n",
        "- Dropping variables: features_to_drop=['EnclosedPorch', 'GarageYrBlt', 'WoodDeckSF']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "dataCleaning_pipeline = Pipeline(\n",
        "    [\n",
        "        (\n",
        "            \"mean\",\n",
        "            MeanMedianImputer(\n",
        "                imputation_method=\"mean\", variables=[\"LotFrontage\", \"BedroomAbvGr\"]\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"median\",\n",
        "            MeanMedianImputer(\n",
        "                imputation_method=\"median\", variables=[\"2ndFlrSF\", \"MasVnrArea\"]\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"categorical\",\n",
        "            CategoricalImputer(\n",
        "                imputation_method=\"missing\",\n",
        "                fill_value=\"None\",\n",
        "                variables=[\"GarageFinish\", \"BsmtFinType1\"],\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"drop\",\n",
        "            DropFeatures(\n",
        "                features_to_drop=[\"EnclosedPorch\", \"GarageYrBlt\", \"WoodDeckSF\"]\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the pipeline to the whole dataset to get cleaned data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet, TestSet = dataCleaning_pipeline.fit_transform(\n",
        "    TrainSet\n",
        "), dataCleaning_pipeline.fit_transform(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = dataCleaning_pipeline.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TrainSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "## Save Cleaned Data and Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "# Create output dirs\n",
        "os.makedirs(\"outputs/datasets/cleaned\", exist_ok=True)\n",
        "# Save CSVs\n",
        "pd.DataFrame(FullCleaned, columns=TrainCleaned.columns).to_csv(\n",
        "    \"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False\n",
        ")\n",
        "pd.DataFrame(TrainCleaned, columns=TrainCleaned.columns).to_csv(\n",
        "    \"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False\n",
        ")\n",
        "pd.DataFrame(TestCleaned, columns=TestCleaned.columns).to_csv(\n",
        "    \"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False\n",
        ")\n",
        "# Save pipeline\n",
        "import joblib\n",
        "\n",
        "os.makedirs(\"outputs/ml_pipeline/data_cleaning\", exist_ok=True)\n",
        "joblib.dump(pipeline, \"outputs/ml_pipeline/data_cleaning/dataCleaning_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Summary**\n",
        "\n",
        "- Assess and handle missing values\n",
        "    - Mean imputation: variables=['LotFrontage' , 'BedroomAbvGr']\n",
        "    - Median imputation: variables=['2ndFlrSF', 'MasVnrArea']\n",
        "    - Categorical imputation: variables=['GarageFinish' , 'BsmtFinType1']\n",
        "    - Dropping variables: features_to_drop=['EnclosedPorch', 'GarageYrBlt', 'WoodDeckSF']\n",
        "- Clean data\n",
        "\n",
        "**Next Steps**:\n",
        "\n",
        "Move to Data Study (EDA) Notebook to analyze feature–target relationships and generate visual insights for the dashboard."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
