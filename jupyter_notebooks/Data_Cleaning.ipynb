{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "- Identify and assess missing values in the dataset\n",
        "- Handle missing data through imputation or removal\n",
        "- Prepare a clean dataset for modeling and further analysis\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- outputs/datasets/collection/HousePrices.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "- Cleaned full dataset: outputs/datasets/cleaned/HousePricesCleaned.csv\n",
        "- Cleaned train/test splits: outputs/datasets/cleaned/TrainSetCleaned.csv, outputs/datasets/cleaned/TestSetCleaned.csv\n",
        "- Data cleaning pipeline: outputs/ml_pipeline/data_cleaning/dataCleaning_pipeline.pkl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "os.chdir(os.path.dirname(current_dir))  # set project root\n",
        "print(\"Current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load Collected Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HousePrices.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Data Cleaning you are interested to check the distribution and shape of a variable with missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"There are no variables with missing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation and PPS Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "        mask[abs(df) < threshold] = True\n",
        "\n",
        "        fig, axes = plt.subplots(figsize=figsize)\n",
        "        sns.heatmap(\n",
        "            df,\n",
        "            annot=True,\n",
        "            xticklabels=True,\n",
        "            yticklabels=True,\n",
        "            mask=mask,\n",
        "            cmap=\"viridis\",\n",
        "            annot_kws={\"size\": font_annot},\n",
        "            ax=axes,\n",
        "            linewidth=0.5,\n",
        "        )\n",
        "        axes.set_yticklabels(df.columns, rotation=0)\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
        "    if len(df.columns) > 1:\n",
        "        mask = np.zeros_like(df, dtype=bool)\n",
        "        mask[abs(df) < threshold] = True\n",
        "        fig, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(\n",
        "            df,\n",
        "            annot=True,\n",
        "            xticklabels=True,\n",
        "            yticklabels=True,\n",
        "            mask=mask,\n",
        "            cmap=\"rocket_r\",\n",
        "            annot_kws={\"size\": font_annot},\n",
        "            linewidth=0.05,\n",
        "            linecolor=\"grey\",\n",
        "        )\n",
        "        plt.ylim(len(df.columns), 0)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\", numeric_only=True)\n",
        "    df_corr_pearson = df.corr(method=\"pearson\", numeric_only=True)\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.filter([\"x\", \"y\", \"ppscore\"]).pivot(\n",
        "        columns=\"x\", index=\"y\", values=\"ppscore\"\n",
        "    )\n",
        "\n",
        "    pps_score_stats = (\n",
        "        pps_matrix_raw.query(\"ppscore < 1\").filter([\"ppscore\"]).describe().T\n",
        "    )\n",
        "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
        "    print(pps_score_stats.round(3))\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "\n",
        "def DisplayCorrAndPPS(\n",
        "    df_corr_pearson,\n",
        "    df_corr_spearman,\n",
        "    pps_matrix,\n",
        "    CorrThreshold,\n",
        "    PPS_Threshold,\n",
        "    figsize=(20, 12),\n",
        "    font_annot=8,\n",
        "):\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\n",
        "        \"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\"\n",
        "    )\n",
        "    print(\n",
        "        \"* Analyse multi-colinearity, that is, how the features are correlated among themselves\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
        "    print(\"It evaluates monotonic relationship \\n\")\n",
        "    heatmap_corr(\n",
        "        df=df_corr_spearman,\n",
        "        threshold=CorrThreshold,\n",
        "        figsize=figsize,\n",
        "        font_annot=font_annot,\n",
        "    )\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
        "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
        "    heatmap_corr(\n",
        "        df=df_corr_pearson,\n",
        "        threshold=CorrThreshold,\n",
        "        figsize=figsize,\n",
        "        font_annot=font_annot,\n",
        "    )\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
        "    print(\n",
        "        f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
        "        f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\"\n",
        "    )\n",
        "    heatmap_pps(\n",
        "        df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate Correlations and Power Predictive Score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display at Heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DisplayCorrAndPPS(\n",
        "    df_corr_pearson=df_corr_pearson,\n",
        "    df_corr_spearman=df_corr_spearman,\n",
        "    pps_matrix=pps_matrix,\n",
        "    CorrThreshold=0.4,\n",
        "    PPS_Threshold=0.2,\n",
        "    figsize=(12, 10),\n",
        "    font_annot=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Custom function to display missing data levels in a DataFrame, it shows the absolute levels, relative levels and data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    \"\"\"\n",
        "    Function to evaluate data with missing values, including most frequent value (mode).\n",
        "    \"\"\"\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute / len(df) * 100, 2)\n",
        "\n",
        "    most_frequent_values = df.apply(\n",
        "        lambda col: (\n",
        "            col.mode(dropna=True)[0] if not col.mode(dropna=True).empty else \"No mode\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    df_missing_data = (\n",
        "        pd.DataFrame(\n",
        "            data={\n",
        "                \"RowsWithMissingData\": missing_data_absolute,\n",
        "                \"PercentageOfDataset\": missing_data_percentage,\n",
        "                \"DataType\": df.dtypes,\n",
        "                \"MostFrequentValue\": most_frequent_values,\n",
        "            }\n",
        "        )\n",
        "        .sort_values(by=[\"PercentageOfDataset\"], ascending=False)\n",
        "        .query(\"PercentageOfDataset > 0\")\n",
        "    )\n",
        "\n",
        "    return df_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Data Evaluation\n",
        "Our goal is to keep as much useful data as possible, while dealing with missing values properly. We looked at each column with missing values and decided whether to drop it or fill it in.\n",
        "\n",
        "**Columns to Drop**\n",
        "These columns are missing too many values, and they don't seem very helpful for predicting the house price. So we will drop them.\n",
        "\n",
        "| Column          | Missing % | Reason                                  |\n",
        "|-----------------|-----------|------------------------------------------|\n",
        "| EnclosedPorch   | 90.68%    | Too many missing values, not very useful |\n",
        "| WoodDeckSF      | 89.38%    | Also many missing values, not so helpful |\n",
        "\n",
        "\n",
        "**Columns to Impute**\n",
        "These columns are more useful, and the missing values can be filled in with simple methods like the most common value or the average/median.\n",
        "Findings:\n",
        "\n",
        "| Column          | Missing % | Fill With        | Reason |\n",
        "|-----------------|-----------|------------------|--------|\n",
        "| LotFrontage     | 17.74%    | Median           | Good for numbers like this |\n",
        "| GarageFinish    | 16.10%    | 'unf' (unfinished) | Most common value |\n",
        "| BsmtFinType1    | 9.93%     | 'unf'            | Most common value |\n",
        "| BedroomAbvGr    | 6.78%     | Median           | Safe and simple |\n",
        "| 2ndFlrSF        | 5.89%     | 0                | Most houses have no 2nd floor |\n",
        "| GarageYrBlt     | 5.55%     | Median           | Close to house build year |\n",
        "| BsmtExposure    | 2.60%     | 'no'             | Most common value |\n",
        "| MasVnrArea      | 0.55%     | 0                | Most houses have 0 here |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop Unnecessary Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to assess the effect of dropped data for each of the imputations we are about to make, we use CI's custom code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DataCleaningEffect(df_original, df_cleaned, variables_applied_with_method):\n",
        "\n",
        "    flag_count = 1  # Indicate plot number\n",
        "\n",
        "    # distinguish between numerical and categorical variables\n",
        "    categorical_variables = df_original.select_dtypes(exclude=[\"number\"]).columns\n",
        "\n",
        "    # scan over variables,\n",
        "    # first on variables that you applied the method\n",
        "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
        "    for set_of_variables in [variables_applied_with_method]:\n",
        "        print(\n",
        "            \"\\n=====================================================================================\"\n",
        "        )\n",
        "        print(\n",
        "            f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\"\n",
        "        )\n",
        "        print(f\"{set_of_variables} \\n\\n\")\n",
        "\n",
        "        for var in set_of_variables:\n",
        "            if var in categorical_variables:  # it is categorical variable: barplot\n",
        "\n",
        "                df1 = pd.DataFrame({\"Type\": \"Original\", \"Value\": df_original[var]})\n",
        "                df2 = pd.DataFrame({\"Type\": \"Cleaned\", \"Value\": df_cleaned[var]})\n",
        "                dfAux = pd.concat([df1, df2], axis=0)\n",
        "                fig, axes = plt.subplots(figsize=(15, 5))\n",
        "                sns.countplot(\n",
        "                    hue=\"Type\", data=dfAux, x=\"Value\", palette=[\"#432371\", \"#FAAE7B\"]\n",
        "                )\n",
        "                axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "                plt.xticks(rotation=90)\n",
        "                plt.legend()\n",
        "\n",
        "            else:  # it is numerical variable: histogram\n",
        "\n",
        "                fig, axes = plt.subplots(figsize=(10, 5))\n",
        "                sns.histplot(\n",
        "                    data=df_original,\n",
        "                    x=var,\n",
        "                    color=\"#432371\",\n",
        "                    label=\"Original\",\n",
        "                    kde=True,\n",
        "                    element=\"step\",\n",
        "                    ax=axes,\n",
        "                )\n",
        "                sns.histplot(\n",
        "                    data=df_cleaned,\n",
        "                    x=var,\n",
        "                    color=\"#FAAE7B\",\n",
        "                    label=\"Cleaned\",\n",
        "                    kde=True,\n",
        "                    element=\"step\",\n",
        "                    ax=axes,\n",
        "                )\n",
        "                axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "                plt.legend()\n",
        "\n",
        "            plt.show()\n",
        "            flag_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "variables_method = [\"EnclosedPorch\", \"WoodDeckSF\"]\n",
        "variables_method\n",
        "\n",
        "imputer = DropFeatures(features_to_drop=variables_method)\n",
        "df_method = imputer.fit_transform(df)\n",
        "df_method.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Impute Variables (handle missing data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Arbitrary Number Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The number '0' shall be imputed for 2ndFlrSF and MasVnrArea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import ArbitraryNumberImputer\n",
        "\n",
        "variables_method = [\"2ndFlrSF\", \"MasVnrArea\"]\n",
        "variables_method\n",
        "\n",
        "imputer = ArbitraryNumberImputer(arbitrary_number=0, variables=variables_method)\n",
        "df_method = imputer.fit_transform(df)\n",
        "\n",
        "DataCleaningEffect(\n",
        "    df_original=df, df_cleaned=df_method, variables_applied_with_method=variables_method\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the cleaned data has a similar distribution to the original data, indicating no major distortion by the cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Median Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The median shall be imputed for LotFrontage, BedroomAbvGr and GarageYrBlt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "variables_method = [\"LotFrontage\",\"BedroomAbvGr\",\"GarageYrBlt\"]\n",
        "variables_method\n",
        "\n",
        "imputer = MeanMedianImputer(imputation_method=\"median\", variables=variables_method)\n",
        "df_method = imputer.fit_transform(df)\n",
        "\n",
        "DataCleaningEffect(\n",
        "    df_original=df, df_cleaned=df_method, variables_applied_with_method=variables_method\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the distributions of the cleaned data are not identical to the original, they remain fairly consistent overall. Since the visual comparison shows no significant distortion, we consider the cleaning process appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The value 'Unf', due to being the most common one, shall be imputed for BsmtFinType1 and GarageFinish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "variables_method = [\"BsmtFinType1\", \"GarageFinish\"]\n",
        "variables_method\n",
        "\n",
        "imputer = CategoricalImputer(\n",
        "    imputation_method=\"missing\", fill_value=\"Unf\", variables=variables_method\n",
        ")\n",
        "df_method = imputer.fit_transform(df)\n",
        "\n",
        "DataCleaningEffect(\n",
        "    df_original=df, df_cleaned=df_method, variables_applied_with_method=variables_method\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the cleaned data shows more values of 'Unf', which is what the imputation was supposed to achieve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The value 'No', due to being the most common one, shall be imputed for BsmtExposure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "variables_method = [\"BsmtExposure\"]\n",
        "variables_method\n",
        "\n",
        "imputer = CategoricalImputer(\n",
        "    imputation_method=\"missing\", fill_value=\"No\", variables=variables_method\n",
        ")\n",
        "df_method = imputer.fit_transform(df)\n",
        "\n",
        "DataCleaningEffect(\n",
        "    df_original=df, df_cleaned=df_method, variables_applied_with_method=variables_method\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the final cleaned df_method into Train/Test\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "    df_method, df_method[\"SalePrice\"], test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.isnull().sum().sort_values(ascending=False).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- These cleaned datasets can now be used for Feature Engineering or Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the pipeline to the whole dataset to get cleaned data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from feature_engine.imputation import (\n",
        "    MeanMedianImputer,\n",
        "    ArbitraryNumberImputer,\n",
        "    CategoricalImputer,\n",
        ")\n",
        "\n",
        "cleaning_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\n",
        "            \"median_imputer\",\n",
        "            MeanMedianImputer(\n",
        "                imputation_method=\"median\",\n",
        "                variables=[\"LotFrontage\", \"GarageYrBlt\", \"BedroomAbvGr\"],\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"arbitrary_imputer\",\n",
        "            ArbitraryNumberImputer(\n",
        "                arbitrary_number=0, variables=[\"2ndFlrSF\", \"MasVnrArea\"]\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"cat_imputer_unf\",\n",
        "            CategoricalImputer(\n",
        "                imputation_method=\"missing\",\n",
        "                fill_value=\"Unf\",\n",
        "                variables=[\"GarageFinish\", \"BsmtFinType1\"],\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"cat_imputer_no\",\n",
        "            CategoricalImputer(\n",
        "                imputation_method=\"missing\", fill_value=\"No\", variables=[\"BsmtExposure\"]\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply the Pipeline to Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the pipeline only on training data\n",
        "cleaning_pipeline.fit(X_train)\n",
        "\n",
        "# Transform both train and test sets\n",
        "X_train_cleaned = cleaning_pipeline.transform(X_train)\n",
        "X_test_cleaned = cleaning_pipeline.transform(X_test)\n",
        "\n",
        "# Optional: combine with target\n",
        "TrainSetCleaned = X_train_cleaned.copy()\n",
        "TrainSetCleaned[\"SalePrice\"] = y_train.values\n",
        "\n",
        "TestSetCleaned = X_test_cleaned.copy()\n",
        "TestSetCleaned[\"SalePrice\"] = y_test.values\n",
        "\n",
        "FullCleaned = pd.concat([TrainSetCleaned, TestSetCleaned], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "## Save Cleaned Data and Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "\n",
        "# Create output folders\n",
        "os.makedirs(\"outputs/datasets/cleaned\", exist_ok=True)\n",
        "os.makedirs(\"outputs/ml_pipeline/data_cleaning\", exist_ok=True)\n",
        "\n",
        "# Save cleaned data\n",
        "TrainSetCleaned.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)\n",
        "TestSetCleaned.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)\n",
        "FullCleaned.to_csv(\"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False)\n",
        "\n",
        "# Save pipeline\n",
        "joblib.dump(\n",
        "    cleaning_pipeline, \"outputs/ml_pipeline/data_cleaning/dataCleaning_pipeline.pkl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Summary**\n",
        "\n",
        "- Assessed and handled missing values:\n",
        "    - Median imputation: variables = ['LotFrontage', 'GarageYrBlt', 'BedroomAbvGr']\n",
        "    - Arbitrary (0) imputation: variables = ['2ndFlrSF', 'MasVnrArea']\n",
        "    - Categorical imputation (most frequent):\n",
        "        - fill_value='Unf': variables = ['GarageFinish', 'BsmtFinType1']\n",
        "        - fill_value='No' : variables = ['BsmtExposure']\n",
        "    - Dropped variables: features_to_drop = ['EnclosedPorch', 'WoodDeckSF']\n",
        "- Split dataset into training and test sets\n",
        "- Built a cleaning pipeline for future reuse\n",
        "- Cleaned data saved to `outputs/datasets/cleaned/`\n",
        "- Data cleaning pipeline saved to `outputs/ml_pipeline/data_cleaning/`\n",
        "\n",
        "**Next Steps**:\n",
        "\n",
        "Move to Data Study (EDA) Notebook to analyze featureâ€“target relationships and generate visual insights for the dashboard."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
